{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data available through Maize GDB (Maize Genetics and Genomics Database)\n",
    "The purpose of this notebook is to read in and do a preliminary analysis of the data related to text descriptions that are available through Maize GDB. The data was provided in the form of the input file by a request through Maize GDB curators, rather than obtained through an already available file from the database. The data needs to be organized and also restructured into a standard format that will allow it to be easily combined with datasets from other resources. This notebook takes the following input files that were obtained from MaizeGDB and produces a set of files that have standard columns that are listed and described below.\n",
    "\n",
    "### Files read\n",
    "```\n",
    "plant-data/databases/maizegdb/pheno_genes.txt\n",
    "plant-data/databases/maizegdb/maize_v3.gold.gaf\n",
    "```\n",
    "\n",
    "\n",
    "### Files created\n",
    "```\n",
    "plant-data/reshaped_data/maizegdb_phenotype_descriptions.csv\n",
    "plant-data/reshaped_data/maizegdb_go_annotations.csv\n",
    "```\n",
    "\n",
    "### Columns in the created files\n",
    "* **species**: A string indicating what species the gene is in, currently uses the 3-letter codes from the KEGG database.\n",
    "* **unique_gene_identifiers**: Pipe delimited list of gene identifiers, names, models, etc which must uniquely refer to this gene.\n",
    "* **other_gene_identifiers**: Pipe delimited list of other identifiers, names, aliases, synonyms for the gene, which may but do not have to uniquely refer to it.\n",
    "* **gene_models**: Pipe delimited list of gene model names that map to this gene.\n",
    "* **descriptions**: A free text field for any descriptions of phenotypes associated with this gene.\n",
    "* **annotations**: Pipe delimited list of gene ontology term identifiers.\n",
    "* **sources**: Pipe delimited list of strings that indicate where this data comes from such as database names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-96313916d579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../oats\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcatenate_with_delim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtract_string_lists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_delimiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_enclosing_brackets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_safely\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/triffid/prasanth/reorganizing-irb-scripts/oats/oats/nlp/preprocess.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_punctuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_multiple_whitespaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from constants import NCBI_TAG, UNIPROT_TAG, EVIDENCE_CODES\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.nlp.preprocess import concatenate_with_delim, subtract_string_lists, replace_delimiter, concatenate_texts\n",
    "from oats.nlp.small import remove_punctuation, remove_enclosing_brackets, add_prefix_safely\n",
    "\n",
    "OUTPUT_DIR = \"../reshaped_data\"\n",
    "mpl.rcParams[\"figure.dpi\"] = 200\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0.tar.gz (23.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.2 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /work/triffid/prasanth/reorganizing-irb-scripts/anaconda3/envs/corn/lib/python3.6/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /work/triffid/prasanth/reorganizing-irb-scripts/anaconda3/envs/corn/lib/python3.6/site-packages (from gensim) (1.5.4)\n",
      "Collecting smart_open>=1.8.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 1.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gensim\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25l-"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that should be in the final reshaped files.\n",
    "reshaped_columns = [\"species\", \n",
    " \"unique_gene_identifiers\", \n",
    " \"other_gene_identifiers\", \n",
    " \"gene_models\", \n",
    " \"descriptions\", \n",
    " \"annotations\", \n",
    " \"sources\"]\n",
    "\n",
    "# Creating and testing a lambda for finding gene model strings.\n",
    "gene_model_pattern_1 = re.compile(\"grmzm.+\")\n",
    "gene_model_pattern_2 = re.compile(\"zm[0-9]+d[0-9]+\")\n",
    "is_gene_model = lambda s: bool(gene_model_pattern_1.match(s.lower() or gene_model_pattern_2.match(s.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File with genes and phenotype descriptions (pheno_genes.txt)\n",
    "Note that fillna is being used here to replace missing values with an empty string. This is done so that the missing string will be quantified when checking for the number of occurences of unique values from different columns, see the analysis below. However this is not necessary as a preprocessing step because when the data is read in and appended to a dataset object later, any missing values or empty strings will be handled at that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../databases/maizegdb/pheno_genes.txt\"\n",
    "usecols = [\"phenotype_name\", \"phenotype_description\", \"locus_name\", \"alleles\", \"locus_synonyms\", \"v3_gene_model\", \"v4_gene_model\", \"uniprot_id\", \"ncbi_gene\"]\n",
    "df = pd.read_table(filename, usecols=usecols)\n",
    "df.fillna(\"\", inplace=True)\n",
    "print(df[[\"phenotype_name\",\"phenotype_description\"]].head(10))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text information about the phenotypes are contained in both the phenotype name and phenotype description for these data. The can be concatenated and retained together in a new description column that contains all this information, or just the phenotype description could be retained, depending on which data should be used downstream for making similarity comparisons. This is different than for most of the other sources of text used. The next cell looks at how many unique values there are in this data for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out how many unique values there are for each column.\n",
    "unique_values = {col:len(pd.unique(df[col].values)) for col in df.columns}\n",
    "for k,v in unique_values.items():\n",
    "    print(\"{:24}{:8}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a fairly small number of distinct phenotype descriptions (379) compared to the number of lines that are in the complete dataset (3,616). This means that the same descriptions is occuring many times. Look at which descriptions are occuring most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list sorted by number of occurences for each phenotype description.\n",
    "description_counts = df[\"phenotype_description\"].value_counts().to_dict()\n",
    "sorted_tuples = sorted(description_counts.items(), key = lambda x: x[1], reverse=True)\n",
    "for t in sorted_tuples[0:10]:\n",
    "    print(\"{:6}    {:20}\".format(t[1],t[0][:70]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only description that occurs far more often than the next is an empty string, where this information is missing entirely. The next cell looks at how many phrases are included in the phenotype description values. Most have a single phrase, some have multiple. These look like they are mainly separated with semicolons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distributions of number of phrases in each description.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.set_title(\"Phenotype Descriptions\")\n",
    "ax2.set_title(\"Phenotype Descriptions\")\n",
    "ax1.set_xlabel(\"Number of phrases\")\n",
    "ax2.set_xlabel(\"Number of words\")\n",
    "x1 = [len(sent_tokenize(x)) for x in df[\"phenotype_description\"].values]\n",
    "x2 = [len(word_tokenize(x)) for x in df[\"phenotype_description\"].values]\n",
    "ax1.hist(x1, bins=15, range=(0,15), density=False, alpha=0.8, histtype='stepfilled', color=\"black\", edgecolor='none')\n",
    "ax2.hist(x2, bins=30, range=(0,150), density=False, alpha=0.8, histtype='stepfilled', color=\"black\", edgecolor='none')\n",
    "fig.set_size_inches(15,4)\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructuring the dataset to include all the expected column names.\n",
    "combine_columns = lambda row, columns: concatenate_with_delim(\"|\", [row[column] for column in columns])\n",
    "df[\"descriptions\"] = df.apply(lambda x: combine_columns(x, [\"phenotype_name\", \"phenotype_description\"]), axis=1)\n",
    "df[\"uniprot_id\"] = df[\"uniprot_id\"].apply(add_prefix_safely, prefix=UNIPROT_TAG)\n",
    "df[\"ncbi_gene\"] = df[\"ncbi_gene\"].apply(add_prefix_safely, prefix=NCBI_TAG)\n",
    "df[\"unique_gene_identifiers\"] = df.apply(lambda x: combine_columns(x, [\"locus_name\", \"alleles\", \"v3_gene_model\", \"v4_gene_model\", \"uniprot_id\", \"ncbi_gene\"]), axis=1)\n",
    "df[\"other_gene_identifiers\"] = df[\"locus_synonyms\"]\n",
    "df[\"gene_models\"] = df.apply(lambda x: combine_columns(x, [\"v3_gene_model\", \"v4_gene_model\"]), axis=1)\n",
    "df[\"species\"] = \"zma\"\n",
    "df[\"annotations\"] = \"\"\n",
    "df[\"sources\"] = \"MaizeGDB\"\n",
    "df[\"other_gene_identifiers\"] = df.apply(lambda row: subtract_string_lists(\"|\", row[\"other_gene_identifiers\"],row[\"unique_gene_identifiers\"]), axis=1)\n",
    "df = df[reshaped_columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting the dataset of descriptions to a csv file.\n",
    "path = os.path.join(OUTPUT_DIR,\"maizegdb_phenotype_descriptions.csv\")\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File with high confidence gene ontology annotations (maize_v3.gold.gaf)\n",
    "This file was generated as part of the [Maize GAMER](https://onlinelibrary.wiley.com/doi/full/10.1002/pld3.52)  publication (Wimalanathan et al., 2018). The annotations include all of the associations between maize genes and ontology terms from GO where the terms have been experimentally confirmed to represent correct functional annotations for those genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../databases/maizegdb/maize_v3.gold.gaf\"\n",
    "df = pd.read_table(filename, skiprows=1)\n",
    "df.fillna(\"\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructuring the dataset to include all the expected column names.\n",
    "df[\"descriptions\"] = \"\"\n",
    "df[\"unique_gene_identifiers\"] = df.apply(lambda x: combine_columns(x, [\"db_object_id\", \"db_object_symbol\"]), axis=1)\n",
    "df[\"other_gene_identifiers\"] = df.apply(lambda x: combine_columns(x, [\"db_object_name\", \"db_object_synonym\"]), axis=1)\n",
    "df[\"gene_models\"] =  df[\"unique_gene_identifiers\"].map(lambda x: \"\".join([s for s in x.split(\"|\") if is_gene_model(s)]))\n",
    "df[\"species\"] = \"zma\"\n",
    "df[\"annotations\"] = df[\"term_accession\"]\n",
    "df[\"sources\"] = \"MaizeGDB\"\n",
    "df = df[reshaped_columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting the dataset of annotations to a csv file.\n",
    "path = os.path.join(OUTPUT_DIR,\"maizegdb_curated_go_annotations.csv\")\n",
    "df.to_csv(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
